{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPn+1dcMrNp3UGWALzpoPoR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LucasEPrz/APRENDIZAJE-POR-REFUERZO/blob/main/Lab2/Lab2_Perez.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Actividad 1**\n",
        "\n",
        "Crear tu propio entorno y entrenar agentes RL en el mismo. Analizar la convergencia con distintos algoritmos* (ej: PPO, DQN), resultados con distintas funciones de recompensa e híper-parámetros.\n",
        "\n",
        "Algunas ideas:\n",
        "\n",
        "Transformar GoLeftEnv en una grilla 2D, añadir paredes / trampas / agua.\n",
        "Crear un entorno que juegue a algún juego como el ta-te-ti.\n",
        "Crea un entorno totalmente nuevo que sea de tu interés!"
      ],
      "metadata": {
        "id": "Lx7G4_oiKvrL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Creación del entorno**"
      ],
      "metadata": {
        "id": "AY9bAaTyLBkw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definir la estructura del entorno"
      ],
      "metadata": {
        "id": "16Fu2zDOLUX9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from gym import spaces\n",
        "import numpy as np\n",
        "\n",
        "class CustomGridEnv(gym.Env):\n",
        "    def __init__(self):\n",
        "        super(CustomGridEnv, self).__init__()\n",
        "\n",
        "        self.grid_size = 5  # Tamaño de la grilla (5x5)\n",
        "        self.agent_pos = [0, 0]  # Posición inicial del agente (en la esquina superior izquierda)\n",
        "        self.goal_pos = [4, 4]  # Posición del objetivo (en la esquina inferior derecha)\n",
        "\n",
        "        # Definir la acción: [arriba, abajo, izquierda, derecha]\n",
        "        self.action_space = spaces.Discrete(4)\n",
        "\n",
        "        # Definir el espacio de observación: matriz 5x5, donde 0 = vacío, 1 = pared, 2 = trampa, 3 = objetivo\n",
        "        self.observation_space = spaces.Box(low=0, high=3, shape=(self.grid_size, self.grid_size), dtype=int)\n",
        "\n",
        "        # Mapa con paredes y trampas\n",
        "        self.grid = np.zeros((self.grid_size, self.grid_size))\n",
        "        self.grid[1, 1] = 1  # Pared en (1,1)\n",
        "        self.grid[2, 2] = 2  # Trampa en (2,2)\n",
        "\n",
        "    def reset(self):\n",
        "        self.agent_pos = [0, 0]  # Reiniciar la posición del agente\n",
        "        return self.grid  # Devuelve el estado inicial de la grilla\n",
        "\n",
        "    def step(self, action):\n",
        "        # Mover al agente en función de la acción\n",
        "        if action == 0:  # Arriba\n",
        "            self.agent_pos[0] = max(0, self.agent_pos[0] - 1)\n",
        "        elif action == 1:  # Abajo\n",
        "            self.agent_pos[0] = min(self.grid_size - 1, self.agent_pos[0] + 1)\n",
        "        elif action == 2:  # Izquierda\n",
        "            self.agent_pos[1] = max(0, self.agent_pos[1] - 1)\n",
        "        elif action == 3:  # Derecha\n",
        "            self.agent_pos[1] = min(self.grid_size - 1, self.agent_pos[1] + 1)\n",
        "\n",
        "        # Verificar si el agente ha llegado al objetivo o ha caído en una trampa\n",
        "        done = False\n",
        "        reward = -1  # Penalización por cada paso\n",
        "        if self.agent_pos == self.goal_pos:\n",
        "            reward = 10  # Recompensa por llegar al objetivo\n",
        "            done = True\n",
        "        elif self.grid[self.agent_pos[0], self.agent_pos[1]] == 2:  # Si cae en una trampa\n",
        "            reward = -10  # Recompensa negativa por caer en una trampa\n",
        "            done = True\n",
        "\n",
        "        return self.grid, reward, done, {}\n",
        "\n",
        "    def render(self):\n",
        "        # Imprimir la grilla para visualizar el entorno\n",
        "        grid_copy = self.grid.copy()\n",
        "        grid_copy[self.agent_pos[0], self.agent_pos[1]] = 5  # Representar al agente con un 5\n",
        "        print(grid_copy)\n"
      ],
      "metadata": {
        "id": "-SKiN6x2LNWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Entrenar el agente con un algoritmo de RL**"
      ],
      "metadata": {
        "id": "lPj7TA52LcLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install 'shimmy>=2.0'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sg6fS_4eNh2s",
        "outputId": "ae60be03-fff6-45a6-beb3-5fe79e5111d5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting shimmy>=2.0\n",
            "  Downloading Shimmy-2.0.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from shimmy>=2.0) (1.26.4)\n",
            "Collecting gymnasium>=1.0.0a1 (from shimmy>=2.0)\n",
            "  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=1.0.0a1->shimmy>=2.0) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=1.0.0a1->shimmy>=2.0) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium>=1.0.0a1->shimmy>=2.0)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Downloading Shimmy-2.0.0-py3-none-any.whl (30 kB)\n",
            "Downloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium, shimmy\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-1.0.0 shimmy-2.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stable-baselines3[extra]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8s9gs5CLhrW",
        "outputId": "bb782474-573d-4d39-e0ec-c87e2cd0fd63"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting stable-baselines3[extra]\n",
            "  Downloading stable_baselines3-2.4.0-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: gymnasium<1.1.0,>=0.29.1 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (1.0.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.20 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (1.26.4)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.5.1+cu121)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (3.1.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (3.8.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (4.10.0.84)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.6.1)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.17.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (5.9.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (4.66.6)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (13.9.4)\n",
            "Collecting ale-py>=0.9.0 (from stable-baselines3[extra])\n",
            "  Downloading ale_py-0.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (11.0.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from ale-py>=0.9.0->stable-baselines3[extra]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium<1.1.0,>=0.29.1->stable-baselines3[extra]) (0.0.4)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.68.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (4.25.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (75.1.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.1.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.13->stable-baselines3[extra]) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (4.55.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3[extra]) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3[extra]) (2024.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->stable-baselines3[extra]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->stable-baselines3[extra]) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]) (3.0.2)\n",
            "Downloading ale_py-0.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading stable_baselines3-2.4.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ale-py, stable-baselines3\n",
            "Successfully installed ale-py-0.10.1 stable-baselines3-2.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entrenar el agente con PPO"
      ],
      "metadata": {
        "id": "7WFsn2Y1N7h8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "\n",
        "# Crear el entorno\n",
        "env = CustomGridEnv()\n",
        "\n",
        "# Vectorizar el entorno\n",
        "env = DummyVecEnv([lambda: env])\n",
        "\n",
        "# Crear y entrenar el agente PPO\n",
        "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
        "model.learn(total_timesteps=10000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMlKIdj7Lo5z",
        "outputId": "c090b2a0-883c-4c6e-ae14-457737e15a0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    fps             | 1026 |\n",
            "|    iterations      | 1    |\n",
            "|    time_elapsed    | 1    |\n",
            "|    total_timesteps | 2048 |\n",
            "-----------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 668        |\n",
            "|    iterations           | 2          |\n",
            "|    time_elapsed         | 6          |\n",
            "|    total_timesteps      | 4096       |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01902721 |\n",
            "|    clip_fraction        | 0.31       |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.37      |\n",
            "|    explained_variance   | 0          |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 13         |\n",
            "|    n_updates            | 10         |\n",
            "|    policy_gradient_loss | -0.0263    |\n",
            "|    value_loss           | 75.7       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 664         |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 9           |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018353956 |\n",
            "|    clip_fraction        | 0.334       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.32       |\n",
            "|    explained_variance   | -1.19e-07   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 38.8        |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.0254     |\n",
            "|    value_loss           | 76.1        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 662         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 12          |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015977047 |\n",
            "|    clip_fraction        | 0.256       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.25       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 43.3        |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.0203     |\n",
            "|    value_loss           | 84.1        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 654         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 15          |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014817617 |\n",
            "|    clip_fraction        | 0.158       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.17       |\n",
            "|    explained_variance   | -1.19e-07   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 31.6        |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.0193     |\n",
            "|    value_loss           | 76.9        |\n",
            "-----------------------------------------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<stable_baselines3.ppo.ppo.PPO at 0x7c30e8c66e00>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analizar la convergencia"
      ],
      "metadata": {
        "id": "Bk_0f4syONOn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = PPO(\"MlpPolicy\", env, verbose=1, learning_rate=0.001, n_steps=256, gamma=0.99, ent_coef=0.01)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnIi6s2VOL8O",
        "outputId": "ee608a10-6cbb-418f-a769-9fc5d7ab9ee8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluación del agente"
      ],
      "metadata": {
        "id": "RLusZMp1OmXG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "state = env.reset()\n",
        "for _ in range(1000):\n",
        "    action, _states = model.predict(state)\n",
        "    state, reward, done, info = env.step(action)\n",
        "    env.render()\n",
        "    if done:\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKm0E8ytOpIU",
        "outputId": "5339a400-193e-4019-bcdd-3732d329b402"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/vec_env/base_vec_env.py:243: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.\n",
            "  warnings.warn(\"You tried to call render() but no `render_mode` was passed to the env constructor.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Probar con otros algoritmos (DQN)"
      ],
      "metadata": {
        "id": "i5zkIkZ3OvbK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3 import DQN\n",
        "\n",
        "# Entrenar con DQN\n",
        "model_dqn = DQN(\"MlpPolicy\", env, verbose=1)\n",
        "model_dqn.learn(total_timesteps=10000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6crt3Bh8O0Cx",
        "outputId": "c6a5ca3e-999c-4d32-e097-62bedd23e969"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.578    |\n",
            "| time/               |          |\n",
            "|    episodes         | 4        |\n",
            "|    fps              | 1391     |\n",
            "|    time_elapsed     | 0        |\n",
            "|    total_timesteps  | 444      |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.687    |\n",
            "|    n_updates        | 85       |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 8        |\n",
            "|    fps              | 985      |\n",
            "|    time_elapsed     | 2        |\n",
            "|    total_timesteps  | 2528     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.000332 |\n",
            "|    n_updates        | 606      |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 12       |\n",
            "|    fps              | 917      |\n",
            "|    time_elapsed     | 4        |\n",
            "|    total_timesteps  | 4542     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.56e-07 |\n",
            "|    n_updates        | 1110     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16       |\n",
            "|    fps              | 820      |\n",
            "|    time_elapsed     | 7        |\n",
            "|    total_timesteps  | 6490     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.59e-06 |\n",
            "|    n_updates        | 1597     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 20       |\n",
            "|    fps              | 828      |\n",
            "|    time_elapsed     | 8        |\n",
            "|    total_timesteps  | 7123     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.28e-07 |\n",
            "|    n_updates        | 1755     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 24       |\n",
            "|    fps              | 839      |\n",
            "|    time_elapsed     | 9        |\n",
            "|    total_timesteps  | 7995     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.76e-06 |\n",
            "|    n_updates        | 1973     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 28       |\n",
            "|    fps              | 846      |\n",
            "|    time_elapsed     | 10       |\n",
            "|    total_timesteps  | 8734     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 6.85e-07 |\n",
            "|    n_updates        | 2158     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 32       |\n",
            "|    fps              | 849      |\n",
            "|    time_elapsed     | 11       |\n",
            "|    total_timesteps  | 9411     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.52e-06 |\n",
            "|    n_updates        | 2327     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 36       |\n",
            "|    fps              | 853      |\n",
            "|    time_elapsed     | 11       |\n",
            "|    total_timesteps  | 9972     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 6.89e-07 |\n",
            "|    n_updates        | 2467     |\n",
            "----------------------------------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<stable_baselines3.dqn.dqn.DQN at 0x7c2fbd750b80>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Graficar la convergencia"
      ],
      "metadata": {
        "id": "GgjXNXjQO6k2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3.common.vec_env import VecNormalize\n",
        "from stable_baselines3.common.callbacks import EvalCallback\n",
        "\n",
        "eval_callback = EvalCallback(env, best_model_save_path='./logs/', log_path='./logs/', eval_freq=5000, deterministic=True, render=False)\n",
        "\n",
        "# Entrenar el modelo con el callback\n",
        "model.learn(total_timesteps=10000, callback=eval_callback)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gzs0RyFVO8wc",
        "outputId": "6a0a44cb-cf75-4ddd-fb88-c5beb7400cae"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    fps             | 1187 |\n",
            "|    iterations      | 1    |\n",
            "|    time_elapsed    | 0    |\n",
            "|    total_timesteps | 256  |\n",
            "-----------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 846         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 0           |\n",
            "|    total_timesteps      | 512         |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015029857 |\n",
            "|    clip_fraction        | 0.187       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.02       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 28.4        |\n",
            "|    n_updates            | 70          |\n",
            "|    policy_gradient_loss | -0.0203     |\n",
            "|    value_loss           | 57.1        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 766         |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 1           |\n",
            "|    total_timesteps      | 768         |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016991742 |\n",
            "|    clip_fraction        | 0.0816      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.955      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 21.9        |\n",
            "|    n_updates            | 80          |\n",
            "|    policy_gradient_loss | -0.00212    |\n",
            "|    value_loss           | 42.8        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 729         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 1           |\n",
            "|    total_timesteps      | 1024        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010240927 |\n",
            "|    clip_fraction        | 0.143       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.963      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 19.5        |\n",
            "|    n_updates            | 90          |\n",
            "|    policy_gradient_loss | -0.0098     |\n",
            "|    value_loss           | 43.5        |\n",
            "-----------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 692           |\n",
            "|    iterations           | 5             |\n",
            "|    time_elapsed         | 1             |\n",
            "|    total_timesteps      | 1280          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00089663104 |\n",
            "|    clip_fraction        | 0.0297        |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.964        |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.001         |\n",
            "|    loss                 | 24            |\n",
            "|    n_updates            | 100           |\n",
            "|    policy_gradient_loss | -0.00438      |\n",
            "|    value_loss           | 44.7          |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 644          |\n",
            "|    iterations           | 6            |\n",
            "|    time_elapsed         | 2            |\n",
            "|    total_timesteps      | 1536         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0029059965 |\n",
            "|    clip_fraction        | 0.152        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.898       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.001        |\n",
            "|    loss                 | 17           |\n",
            "|    n_updates            | 110          |\n",
            "|    policy_gradient_loss | -0.0129      |\n",
            "|    value_loss           | 34.6         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 620          |\n",
            "|    iterations           | 7            |\n",
            "|    time_elapsed         | 2            |\n",
            "|    total_timesteps      | 1792         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0066154283 |\n",
            "|    clip_fraction        | 0.0824       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.863       |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.001        |\n",
            "|    loss                 | 22.9         |\n",
            "|    n_updates            | 120          |\n",
            "|    policy_gradient_loss | -0.00718     |\n",
            "|    value_loss           | 42.4         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 603          |\n",
            "|    iterations           | 8            |\n",
            "|    time_elapsed         | 3            |\n",
            "|    total_timesteps      | 2048         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0017329413 |\n",
            "|    clip_fraction        | 0.00469      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.866       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.001        |\n",
            "|    loss                 | 15.8         |\n",
            "|    n_updates            | 130          |\n",
            "|    policy_gradient_loss | 7.84e-05     |\n",
            "|    value_loss           | 36.2         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 583         |\n",
            "|    iterations           | 9           |\n",
            "|    time_elapsed         | 3           |\n",
            "|    total_timesteps      | 2304        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010308495 |\n",
            "|    clip_fraction        | 0.0484      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.853      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 19.9        |\n",
            "|    n_updates            | 140         |\n",
            "|    policy_gradient_loss | -0.00545    |\n",
            "|    value_loss           | 41.5        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 568         |\n",
            "|    iterations           | 10          |\n",
            "|    time_elapsed         | 4           |\n",
            "|    total_timesteps      | 2560        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009346618 |\n",
            "|    clip_fraction        | 0.0484      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.849      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 33.4        |\n",
            "|    n_updates            | 150         |\n",
            "|    policy_gradient_loss | -0.00134    |\n",
            "|    value_loss           | 51.6        |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 558        |\n",
            "|    iterations           | 11         |\n",
            "|    time_elapsed         | 5          |\n",
            "|    total_timesteps      | 2816       |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01387301 |\n",
            "|    clip_fraction        | 0.131      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.846     |\n",
            "|    explained_variance   | 0          |\n",
            "|    learning_rate        | 0.001      |\n",
            "|    loss                 | 22.8       |\n",
            "|    n_updates            | 160        |\n",
            "|    policy_gradient_loss | -0.013     |\n",
            "|    value_loss           | 43.3       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 562         |\n",
            "|    iterations           | 12          |\n",
            "|    time_elapsed         | 5           |\n",
            "|    total_timesteps      | 3072        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.024869544 |\n",
            "|    clip_fraction        | 0.115       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.794      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 22.6        |\n",
            "|    n_updates            | 170         |\n",
            "|    policy_gradient_loss | -0.00505    |\n",
            "|    value_loss           | 41.6        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 567         |\n",
            "|    iterations           | 13          |\n",
            "|    time_elapsed         | 5           |\n",
            "|    total_timesteps      | 3328        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006484837 |\n",
            "|    clip_fraction        | 0.0676      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.792      |\n",
            "|    explained_variance   | -1.19e-07   |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 22.3        |\n",
            "|    n_updates            | 180         |\n",
            "|    policy_gradient_loss | -0.00405    |\n",
            "|    value_loss           | 41.5        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 572         |\n",
            "|    iterations           | 14          |\n",
            "|    time_elapsed         | 6           |\n",
            "|    total_timesteps      | 3584        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017563844 |\n",
            "|    clip_fraction        | 0.052       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.8        |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 23.6        |\n",
            "|    n_updates            | 190         |\n",
            "|    policy_gradient_loss | -0.00125    |\n",
            "|    value_loss           | 37.7        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 576         |\n",
            "|    iterations           | 15          |\n",
            "|    time_elapsed         | 6           |\n",
            "|    total_timesteps      | 3840        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004906596 |\n",
            "|    clip_fraction        | 0.0145      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.773      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 21          |\n",
            "|    n_updates            | 200         |\n",
            "|    policy_gradient_loss | -0.000811   |\n",
            "|    value_loss           | 41.3        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 581          |\n",
            "|    iterations           | 16           |\n",
            "|    time_elapsed         | 7            |\n",
            "|    total_timesteps      | 4096         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0040554805 |\n",
            "|    clip_fraction        | 0.0637       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.78        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.001        |\n",
            "|    loss                 | 19.8         |\n",
            "|    n_updates            | 210          |\n",
            "|    policy_gradient_loss | -0.00136     |\n",
            "|    value_loss           | 37.6         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 585         |\n",
            "|    iterations           | 17          |\n",
            "|    time_elapsed         | 7           |\n",
            "|    total_timesteps      | 4352        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.021610634 |\n",
            "|    clip_fraction        | 0.191       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.743      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 20.5        |\n",
            "|    n_updates            | 220         |\n",
            "|    policy_gradient_loss | -0.013      |\n",
            "|    value_loss           | 39          |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 588         |\n",
            "|    iterations           | 18          |\n",
            "|    time_elapsed         | 7           |\n",
            "|    total_timesteps      | 4608        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.023507591 |\n",
            "|    clip_fraction        | 0.225       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.766      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 16.4        |\n",
            "|    n_updates            | 230         |\n",
            "|    policy_gradient_loss | -0.0216     |\n",
            "|    value_loss           | 34          |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 591         |\n",
            "|    iterations           | 19          |\n",
            "|    time_elapsed         | 8           |\n",
            "|    total_timesteps      | 4864        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.021189421 |\n",
            "|    clip_fraction        | 0.173       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.778      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.001       |\n",
            "|    loss                 | 12.8        |\n",
            "|    n_updates            | 240         |\n",
            "|    policy_gradient_loss | -0.00608    |\n",
            "|    value_loss           | 40          |\n",
            "-----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "lf1GlgsUJ6Qp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Actividad 2**\n",
        "\n",
        "Entrena agentes en entornos más complejos con librerías de agentes más avanzadas. La idea es que den rienda suelta a su creatividad. Algunas ideas de agentes / entornos:\n",
        "\n",
        "stable-baselines/rl-baselines-zoo (como lo vimos en este notebook).\n",
        "CleanRL.\n",
        "Doom.\n",
        "Arcade.\n",
        "Pueden inspirarse en DQN, Policy Gradient o demás secciones del curso de RL de HuggingFace.\n",
        "También puede ser alguna aplicación o notebook usando Decision transformers (blog de referencia)."
      ],
      "metadata": {
        "id": "am0Ha25DJAc9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade stable-baselines3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjlycitiPYjh",
        "outputId": "e9d5cb76-1467-43d4-af9c-f2e31dc32468"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: stable-baselines3 in /usr/local/lib/python3.10/dist-packages (2.4.0)\n",
            "Requirement already satisfied: gymnasium<1.1.0,>=0.29.1 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (1.0.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.20 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (1.26.4)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (2.5.1+cu121)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (3.1.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium<1.1.0,>=0.29.1->stable-baselines3) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium<1.1.0,>=0.29.1->stable-baselines3) (0.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.13->stable-baselines3) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (4.55.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13->stable-baselines3) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqjOdieAOjP1",
        "outputId": "c97342e5-c663-4009-b09b-20d26eaa85d9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium\n",
            "  Using cached gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (0.0.4)\n",
            "Using cached gymnasium-1.0.0-py3-none-any.whl (958 kB)\n",
            "Installing collected packages: gymnasium\n",
            "Successfully installed gymnasium-1.0.0\n"
          ]
        }
      ]
    },
    {
      "source": [
        "import gymnasium as gym  # Usa gymnasium en lugar de gym\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "\n",
        "# Crear y vectorizar el entorno\n",
        "env = DummyVecEnv([lambda: gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")])\n",
        "\n",
        "# Crear el modelo PPO\n",
        "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
        "\n",
        "# Entrenar el modelo\n",
        "model.learn(total_timesteps=10000)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CT0QUFg5MAw1",
        "outputId": "269ce61a-9b29-482e-8fcc-636febe97aaf"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    fps             | 711  |\n",
            "|    iterations      | 1    |\n",
            "|    time_elapsed    | 2    |\n",
            "|    total_timesteps | 2048 |\n",
            "-----------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 609        |\n",
            "|    iterations           | 2          |\n",
            "|    time_elapsed         | 6          |\n",
            "|    total_timesteps      | 4096       |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00978988 |\n",
            "|    clip_fraction        | 0.126      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.685     |\n",
            "|    explained_variance   | -0.00769   |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 8.35       |\n",
            "|    n_updates            | 10         |\n",
            "|    policy_gradient_loss | -0.0211    |\n",
            "|    value_loss           | 53.6       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 458         |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 13          |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009729741 |\n",
            "|    clip_fraction        | 0.0703      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.665      |\n",
            "|    explained_variance   | 0.0984      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 15.4        |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.0192     |\n",
            "|    value_loss           | 35.5        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 452         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 18          |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009333154 |\n",
            "|    clip_fraction        | 0.09        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.635      |\n",
            "|    explained_variance   | 0.208       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 26.4        |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.0177     |\n",
            "|    value_loss           | 55.4        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 473         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 21          |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008009516 |\n",
            "|    clip_fraction        | 0.0668      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.617      |\n",
            "|    explained_variance   | 0.314       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 16.6        |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.0146     |\n",
            "|    value_loss           | 58.5        |\n",
            "-----------------------------------------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<stable_baselines3.ppo.ppo.PPO at 0x79f98f50c820>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fFbDUsYqRtLn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_rewards = 0\n",
        "episodes = 0\n",
        "\n",
        "obs = env.reset()\n",
        "for _ in range(1000):\n",
        "    action, _states = model.predict(obs)\n",
        "    obs, rewards, done, info = env.step(action)\n",
        "\n",
        "    total_rewards += rewards\n",
        "\n",
        "    if done:\n",
        "        episodes += 1\n",
        "        print(f\"Recompensa total del episodio {episodes}: {total_rewards}\")\n",
        "        obs = env.reset()\n",
        "\n",
        "# Promedio de recompensas por episodio\n",
        "print(f\"Promedio de recompensas por episodio: {total_rewards / episodes}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3GCKAPNRuCI",
        "outputId": "3913412d-17d0-4bfc-dc20-9d60043c9cd0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recompensa total del episodio 1: [184.]\n",
            "Recompensa total del episodio 2: [453.]\n",
            "Recompensa total del episodio 3: [600.]\n",
            "Recompensa total del episodio 4: [776.]\n",
            "Recompensa total del episodio 5: [794.]\n",
            "Recompensa total del episodio 6: [972.]\n",
            "Promedio de recompensas por episodio: [166.66667]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusiones actividad 2**"
      ],
      "metadata": {
        "id": "St83UTmDTDmW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este trabajo, entrené un agente utilizando el algoritmo PPO (Proximal Policy Optimization) en el entorno de CartPole-v1, un clásico problema de control en el que el objetivo es balancear un poste sobre un carrito. Utilicé la librería `Stable-Baselines3` para crear y entrenar el agente, y la evaluación se realizó en un entorno vectorizado con `DummyVecEnv` para simular múltiples entornos de manera eficiente. El entrenamiento consistió en 10,000 pasos de tiempo, donde el agente aprendió a maximizar su recompensa mediante la mejora continua de sus políticas de acción."
      ],
      "metadata": {
        "id": "M3lEAZVWSuRp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Durante la evaluación del modelo, se observó que la recompensa total obtenida por el agente aumentaba con el tiempo. El agente logró recompensas crecientes en cada episodio: comenzó con una recompensa de 184 puntos en el primer episodio y alcanzó un máximo de 972 puntos en el sexto episodio. Este aumento sugiere que el agente fue capaz de aprender y mejorar su desempeño a medida que avanzaba en los episodios.\n",
        "\n",
        "Sin embargo, el promedio de las recompensas por episodio fue de aproximadamente 166.67, lo que indica que, aunque el agente mejoró su desempeño en algunos episodios, la recompensa total por episodio aún mostró variabilidad. Esto podría ser un indicio de que el agente no ha aprendido una política completamente estable y eficiente en este entorno."
      ],
      "metadata": {
        "id": "Nwpg4CcNSyPq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A pesar de que el agente mostró un progreso significativo, el promedio de recompensas por episodio aún está lejos de ser óptimo. Esto podría deberse a que el modelo no ha sido entrenado por un número suficiente de pasos, o a la necesidad de ajustar los hiperparámetros del algoritmo, como la tasa de aprendizaje o la entropía de la política. Además, el entorno de CartPole-v1 es relativamente simple, por lo que la exploración de otros entornos más complejos podría ofrecer más desafíos y una mejor oportunidad para mejorar el rendimiento del agente.\n",
        "\n",
        "En futuras implementaciones, se podría experimentar con un mayor número de pasos de entrenamiento, la modificación de la arquitectura de la red neuronal o el uso de técnicas adicionales como el ajuste de recompensas o estrategias de exploración más avanzadas."
      ],
      "metadata": {
        "id": "mkomnjFVS0x7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En conclusión, el agente entrenado con PPO mostró una mejora considerable en el entorno de CartPole-v1, logrando un desempeño razonable en términos de recompensas. Si bien no alcanzó su máximo potencial, los resultados obtenidos proporcionan una base sólida sobre la cual se pueden realizar mejoras. Las futuras investigaciones podrían centrarse en aumentar la capacidad de exploración del agente, ajustar los hiperparámetros y probar con entornos más complejos para observar un comportamiento más robusto y optimizado."
      ],
      "metadata": {
        "id": "nvzy8tDYS31v"
      }
    }
  ]
}